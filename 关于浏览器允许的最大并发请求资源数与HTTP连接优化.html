<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
  今天逛知乎的时候偶然翻到一个关于浏览器最大并发请求资源数的帖子。有些事我们一直在用的东西，有些可能是我们将来会用到的，一起来总结一下。
## 1.浏览器允许的最大并发请求资源数
何谓最大并发请求资源数？浏览器针对同一时间内、同一域名下同时能够存在多少个请求做出的限制，超出数目的请求会被阻塞。不同浏览器对于最大并发请求资源数的限制也不一样，请看下表
|  浏览器 |  HTTP 1.0 |  HTTP 1.1 |
| ------------ | ------------ | ------------ |
| IE6/7  |      4    |  2 |
|  IE8 |  6 |  6 |
|  IE9 |  10 |  10 |
|  IE10 | 6  |  6 |
|  IE11 |  6 |  6 |
|  chrome/firefox |  6 |  6 |
## 2.导致的问题
如果你有个人建站的经历，那么一定遇到过网站性能优化的问题。对于那些js、css、图片等资源加载缓慢的问题你是怎么解决的？我个人是是将这些静态资源都放在了七牛云上。网站所需的资源加载缓慢可能带宽、是否用了CDN等等因素有关，这些只能靠钱解决的咱就不说了。另一个重要的可以很容易解决的原因就是浏览器最大并发请求资源数的问题，因为请求资源数的固定的，前面的资源没加载完就会阻塞后面的资源加载。说到这里有些人可能已经知道该怎么优化了，但是我们先不说优化的事儿，着什么急嘛！真的是
我个人遇到问题是必须知其然知其所以然的人，这个浏览器为什么要限制最大并发请求资源数？我就奇怪了，吃饱的撑的吗？故意恶心人的不是。别急，有大神给你解释（当然不是我，扒过来的...）
1. 由于 TCP 协议的限制，PC 端只有65536个端口可用以向外部发出连接，而操作系统对半开连接数也有限制以保护操作系统的 TCP\IP 协议栈资源不被迅速耗尽，因此浏览器不好发出太多的 TCP 连接，而是采取用完了之后再重复利用 TCP 连接或者干脆重新建立 TCP 连接的方法。
2.如果采用阻塞的套接字模型来建立连接，同时发出多个连接会导致浏览器不得不多开几个线程，而线程有时候算不得是轻量级资源，毕竟做一次上下文切换开销不小。
3.这是浏览器作为一个有良知的客户端在保护服务器。就像以太网的冲突检测机制，客户端在使用公共资源的时候必须要自行决定一个等待期。当超过2个客户端要使用公共资源时，强势的那个邪恶的客户端可能会导致弱势的客户端完全无法访问公共资源。从前迅雷被喷就是因为它不是一个有良知的客户端，它作为 HTTP 协议客户端没有考虑到服务器的压力，作为 BT 客户端没有考虑到自己回馈上传量的义务。

不管你看没看懂，反正我是看的懵懵懂懂。反正人家这么做肯定是有道理的，毕竟谁都不是傻子会没事找事干。
## 3.解决问题，优化性能
随着咱们前端技术的逐渐成熟，出现了许多新技术。例如`cookie free`,`domain hash`,  `css sprites`, `js/css combine`, `max expires time`, `loading images on demand`等等。这些技术的出现和大量使用都和并发资源数有关，我们一个一个来叨叨。
### cookie free
我们都知道每次当我们向服务器发送请求的时候都会将`cookie`一起发送出去，大家有没有想过这样一个问题？按照普通设计，当网站`cookie`信息有1 KB、网站首页共150个资源时，用户在请求过程中需要发送150 KB的`cookie`信息，在512 Kbps的常见上行带宽下，需要长达3秒左右才能全部发送完毕。 尽管这个过程可以和页面下载不同资源的时间并发，但毕竟对速度造成了影响。 而且这些信息在`js/css/images/flash`等静态资源上，几乎是没有任何必要的。 解决方案是启用和主站不同的域名来放置静态资源，这个新的域里不能写`cookie`,也就是说当`cookie`里面没有写东西的时候它是不会随着请求一起发送到服务器的。这就是所谓的`cookie free`。
### domain hash
刚才提到优化性能的时候我说将静态资源放在七牛云上，其实这就是`domain hash`。既然你限制我在同一个域下的最大并发请求数量，那我多来几个域不就达到我想要的效果了吗？就像你开车限号，那我买它十辆车从位数0-9挂它是个牌不就每天都可以开了嘛，当然我还是别做梦了。正经的说在目前普遍过百的整页请求数的前提下，浏览器提供的仅仅数个并发，对于进行了良好优化甚至是前面有CDN的系统而言，是极大的性能瓶颈。 这也就衍生了domain hash技术来使用多个域名加大并发量（因为浏览器是基于domain的并发控制，而不是page），不过过多的散布会导致DNS解析上付出额外的代价，所以一般也是控制在2-4之间。 这里常见的一个性能小坑是没有机制去确保URL的哈希一致性（即同一个静态资源应该被哈希到同一个域名下），而导致资源被多次下载。
### css sprites
精灵图，雪碧图。这还用说吗？主要目的就是为了减少`http`请求的次数。
### js/css combine
压缩合并js/css。全站的js/css原本并不多，其合并技术的产生却是有着和图片不同的考虑。 由于cs/js通常可能对dom布局甚至是内容造成影响，在浏览器解析上，不连贯的载入是会造成多次重新渲染的。因此，在网站变大需要保持模块化来提高可维护性的前提下，js/css combine也就自然衍生了，同时也是minify、compress等对内容进行多余空格、空行、注释的整理和压缩的技术出现的原因。
### max expires time
随着cookie free和domain hash的引入，网站整体的打开速度将会大大的上一个台阶。 这时我们通常看到的问题是大量的请求由于全站公有header/footer/nav等关系，其对应文件早已在本地缓存里存在了，但为了确保这个内容没有发生修改，浏览器还是需要请求一次服务器，拿到一个304 Not Modified才能放心。 一些比较大型的网站在建立了比较规范的发布制度后，会将大部分静态资源的有效期设置为最长，也就是Cache-Control max-age为10年。 这样设置后，浏览器就再也不会在有缓存的前提下去确认文件是否有修改了。  超长的有效期可以让用户在访问曾访问过的网站或网页时，获得最佳的体验。 带来的复杂性则体现在每次对静态资源进行更新时，必须发布为不同的URL来确保用户重新加载变动的资源。
### loading images on demand
按需加载。很多页面浏览量很大，但其实用户直接很大比例直接就跳走了，第一屏以下的内容用户根本就不感兴趣。 对于超大流量的网站如淘宝、新浪等，这个问题尤其重要。   这个时候一般是通过将图片的src标签设置为一个loading或空白的样式，在用户翻页将图片放入可见区或即将放入可见区时再去载入。 不过这个优化其实和并发资源数的关系就比较小了，只是对一些散布不合理，或第一页底部的资源会有一定的帮助。 主要意图还是降低带宽费用。

总的来说，各类技术都是为了能让用户更快的看到页面进行下一步操作，但却不必将宝贵的资源浪费在没有必要的重复请求、不看的内容上。

<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>今天逛知乎的时候偶然翻到一个关于浏览器最大并发请求资源数的帖子。有些事我们一直在用的东西，有些可能是我们将来会用到的，一起来总结一下。</p>

<h2 id="1">1.浏览器允许的最大并发请求资源数</h2>

<p>何谓最大并发请求资源数？浏览器针对同一时间内、同一域名下同时能够存在多少个请求做出的限制，超出数目的请求会被阻塞。不同浏览器对于最大并发请求资源数的限制也不一样，请看下表
|  浏览器 |  HTTP 1.0 |  HTTP 1.1 |
| ------------ | ------------ | ------------ |
| IE6/7  |      4    |  2 |
|  IE8 |  6 |  6 |
|  IE9 |  10 |  10 |
|  IE10 | 6  |  6 |
|  IE11 |  6 |  6 |
|  chrome/firefox |  6 |  6 |</p>

<h2 id="2">2.导致的问题</h2>

<p>如果你有个人建站的经历，那么一定遇到过网站性能优化的问题。对于那些js、css、图片等资源加载缓慢的问题你是怎么解决的？我个人是是将这些静态资源都放在了七牛云上。网站所需的资源加载缓慢可能带宽、是否用了CDN等等因素有关，这些只能靠钱解决的咱就不说了。另一个重要的可以很容易解决的原因就是浏览器最大并发请求资源数的问题，因为请求资源数的固定的，前面的资源没加载完就会阻塞后面的资源加载。说到这里有些人可能已经知道该怎么优化了，但是我们先不说优化的事儿，着什么急嘛！真的是
我个人遇到问题是必须知其然知其所以然的人，这个浏览器为什么要限制最大并发请求资源数？我就奇怪了，吃饱的撑的吗？故意恶心人的不是。别急，有大神给你解释（当然不是我，扒过来的...）
1. 由于 TCP 协议的限制，PC 端只有65536个端口可用以向外部发出连接，而操作系统对半开连接数也有限制以保护操作系统的 TCP\IP 协议栈资源不被迅速耗尽，因此浏览器不好发出太多的 TCP 连接，而是采取用完了之后再重复利用 TCP 连接或者干脆重新建立 TCP 连接的方法。
2.如果采用阻塞的套接字模型来建立连接，同时发出多个连接会导致浏览器不得不多开几个线程，而线程有时候算不得是轻量级资源，毕竟做一次上下文切换开销不小。
3.这是浏览器作为一个有良知的客户端在保护服务器。就像以太网的冲突检测机制，客户端在使用公共资源的时候必须要自行决定一个等待期。当超过2个客户端要使用公共资源时，强势的那个邪恶的客户端可能会导致弱势的客户端完全无法访问公共资源。从前迅雷被喷就是因为它不是一个有良知的客户端，它作为 HTTP 协议客户端没有考虑到服务器的压力，作为 BT 客户端没有考虑到自己回馈上传量的义务。</p>

<p>不管你看没看懂，反正我是看的懵懵懂懂。反正人家这么做肯定是有道理的，毕竟谁都不是傻子会没事找事干。</p>

<h2 id="3">3.解决问题，优化性能</h2>

<p>随着咱们前端技术的逐渐成熟，出现了许多新技术。例如<code>cookie free</code>,<code>domain hash</code>,  <code>css sprites</code>, <code>js/css combine</code>, <code>max expires time</code>, <code>loading images on demand</code>等等。这些技术的出现和大量使用都和并发资源数有关，我们一个一个来叨叨。</p>

<h3 id="cookiefree">cookie free</h3>

<p>我们都知道每次当我们向服务器发送请求的时候都会将<code>cookie</code>一起发送出去，大家有没有想过这样一个问题？按照普通设计，当网站<code>cookie</code>信息有1 KB、网站首页共150个资源时，用户在请求过程中需要发送150 KB的<code>cookie</code>信息，在512 Kbps的常见上行带宽下，需要长达3秒左右才能全部发送完毕。 尽管这个过程可以和页面下载不同资源的时间并发，但毕竟对速度造成了影响。 而且这些信息在<code>js/css/images/flash</code>等静态资源上，几乎是没有任何必要的。 解决方案是启用和主站不同的域名来放置静态资源，这个新的域里不能写<code>cookie</code>,也就是说当<code>cookie</code>里面没有写东西的时候它是不会随着请求一起发送到服务器的。这就是所谓的<code>cookie free</code>。</p>

<h3 id="domainhash">domain hash</h3>

<p>刚才提到优化性能的时候我说将静态资源放在七牛云上，其实这就是<code>domain hash</code>。既然你限制我在同一个域下的最大并发请求数量，那我多来几个域不就达到我想要的效果了吗？就像你开车限号，那我买它十辆车从位数0-9挂它是个牌不就每天都可以开了嘛，当然我还是别做梦了。正经的说在目前普遍过百的整页请求数的前提下，浏览器提供的仅仅数个并发，对于进行了良好优化甚至是前面有CDN的系统而言，是极大的性能瓶颈。 这也就衍生了domain hash技术来使用多个域名加大并发量（因为浏览器是基于domain的并发控制，而不是page），不过过多的散布会导致DNS解析上付出额外的代价，所以一般也是控制在2-4之间。 这里常见的一个性能小坑是没有机制去确保URL的哈希一致性（即同一个静态资源应该被哈希到同一个域名下），而导致资源被多次下载。</p>

<h3 id="csssprites">css sprites</h3>

<p>精灵图，雪碧图。这还用说吗？主要目的就是为了减少<code>http</code>请求的次数。</p>

<h3 id="jscsscombine">js/css combine</h3>

<p>压缩合并js/css。全站的js/css原本并不多，其合并技术的产生却是有着和图片不同的考虑。 由于cs/js通常可能对dom布局甚至是内容造成影响，在浏览器解析上，不连贯的载入是会造成多次重新渲染的。因此，在网站变大需要保持模块化来提高可维护性的前提下，js/css combine也就自然衍生了，同时也是minify、compress等对内容进行多余空格、空行、注释的整理和压缩的技术出现的原因。</p>

<h3 id="maxexpirestime">max expires time</h3>

<p>随着cookie free和domain hash的引入，网站整体的打开速度将会大大的上一个台阶。 这时我们通常看到的问题是大量的请求由于全站公有header/footer/nav等关系，其对应文件早已在本地缓存里存在了，但为了确保这个内容没有发生修改，浏览器还是需要请求一次服务器，拿到一个304 Not Modified才能放心。 一些比较大型的网站在建立了比较规范的发布制度后，会将大部分静态资源的有效期设置为最长，也就是Cache-Control max-age为10年。 这样设置后，浏览器就再也不会在有缓存的前提下去确认文件是否有修改了。  超长的有效期可以让用户在访问曾访问过的网站或网页时，获得最佳的体验。 带来的复杂性则体现在每次对静态资源进行更新时，必须发布为不同的URL来确保用户重新加载变动的资源。</p>

<h3 id="loadingimagesondemand">loading images on demand</h3>

<p>按需加载。很多页面浏览量很大，但其实用户直接很大比例直接就跳走了，第一屏以下的内容用户根本就不感兴趣。 对于超大流量的网站如淘宝、新浪等，这个问题尤其重要。   这个时候一般是通过将图片的src标签设置为一个loading或空白的样式，在用户翻页将图片放入可见区或即将放入可见区时再去载入。 不过这个优化其实和并发资源数的关系就比较小了，只是对一些散布不合理，或第一页底部的资源会有一定的帮助。 主要意图还是降低带宽费用。</p>

<p>总的来说，各类技术都是为了能让用户更快的看到页面进行下一步操作，但却不必将宝贵的资源浪费在没有必要的重复请求、不看的内容上。</p>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "关于浏览器允许的最大并发请求资源数与HTTP连接优化.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</body>
</html>
